--- 
title: "Biostatistiques"
author: "Z. Coulibali, Agronome, PhD."
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
#bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Notes de cours de Biostatistiques"
---

# Introduction {#chapitre-intro}

Placeholder


## √âtapes du projet en science des donn√©es
## Statistiques ou Science des donn√©es ?
## Table des mati√®res

<!--chapter:end:index.Rmd-->


# La science des donn√©es avec R {#chapitre-intro-a-R}

Placeholder


## Organiser son environnement de travail en R
## Pr√©parer son flux de travail
### Installation classique de R
### L'interpr√©teur de commandes RStudio
### R markdown
## Premiers pas avec R
### Types de donn√©es
### Les collections de donn√©es
#### Les Vecteurs
#### Les Matrices
#### Les Listes
#### Les Tableaux
### Les fonctions
### Les boucles
#### Boucles for
#### Boucles while
### Conditions : if, else if, else
### Installer et charger un module
## Enfin...

<!--chapter:end:02_introar.Rmd-->


# Organisation des donn√©es et op√©rations sur les tableaux {#chapitre-tableaux}

Placeholder


## Les collections de donn√©es
## Organiser un tableau de donn√©es
## Formats de tableau
### *xls* ou *xlsx*
### *csv*
### *json*
### SQLite
### Suggestion
## Entreposer ses donn√©es
## Manipuler des donn√©es en mode tidyverse
### Importer les donn√©es dans la session de travail
### Comment s√©lectionner et filtrer des donn√©es ?
#### S√©lectionner
#### Filtrer
### Le format long et le format large
### Combiner des tableaux
### Op√©rations sur les tableaux
### Exemple (difficile) - (Extra)
### Exporter un tableau
### Aller plus loin dans le tidyverse
## R√©f√©rences

<!--chapter:end:03_tableaux.Rmd-->


# Visualisation {#chapitre-visualisation}

Placeholder


## Pourquoi explorer graphiquement ?
## Publier un graphique
### Cinq qualit√©s d'un bon graphique
## Choisir le type de graphique le plus appropri√©
## Choisir son outils de visualisation
### Approche imp√©rative
### Approche d√©clarative
## Visualisation en R
## Module de base pour les graphiques
## La grammaire graphique ggplot2
## Mon premier ggplot
### Les facettes
### Plusieurs sources de donn√©es
### Exporter avec style
### Nuages de points
### Diagrammes en lignes
### Les histogrammes
### Boxplots
### Les diagrammes en barre
### Exporter un graphique
## Les graphiques comme outil d'exploration des donn√©es
### Des graphiques interactifs !
### Des extensions de ggplot2
### Aller plus loin avec ggplot2
## Choisir les bonnes couleurs
## R√®gles particuli√®res
### Ne tronquez pas inutilement l'axe des $y$
### Utilisez un encrage proportionnel
### Publiez vos donn√©es
### Visitez `www.junkcharts.typepad.com/` de temps √† autre 

<!--chapter:end:04_visualisation.Rmd-->


# Biostatistiques, d√©finitions {#chapitre-biostats1}

Placeholder


## Populations et √©chantillons
## Les variables
### Variables quantitatives
### Variables qualitatives
### Niveau ou √©chelle de mesure
## Les probabilit√©s
## Les distributions
### La distribution binomiale
### La distribution de Poisson
### Distribution uniforme
### Distribution normale

<!--chapter:end:05_biostatsdef.Rmd-->


# Statistiques descriptives {#chapitre-statsdesc}

Placeholder


## Sommaire des donn√©es
## Moyenne et √©cart-type
## Quartiles
## D√©comptes et proportions
## Tests d'hypoth√®ses √† 1 et 2 √©chantillons
### L'hypoth√®se nulle
### Test de t √† un seul √©chantillon
### Intervalle de confiance
### La *p-value*
#### Attention aux mauvaises interpr√©tations des *p-values*
#### Attention au *p-hacking*
### Test de Wilcoxon √† un seul √©chantillon
### Tests de t √† deux √©chantillons
### Enregistrer les r√©sultats d'un test
### Comparaison des variances
### Tests de Wilcoxon √† deux √©chantillons
### Les tests pair√©s
## L'analyse de variance

<!--chapter:end:06_statsdesc.Rmd-->


# Mod√®les statistiques {#chapitre-modelisation}

Placeholder


## Mod√®les √† effets fixes
### Mod√®le lin√©aire univari√© avec variable continue
### Analyse des r√©sidus
### R√©gression multiple
### Mod√®les lin√©aires univari√©s avec variable cat√©gorielle **nominale**
#### L'encodage cat√©goriel
#### Exemple d'application
### Mod√®les lin√©aires univari√©s avec variable cat√©gorielle **ordinale**
### R√©gression multiple √† plusieurs variables
### Les mod√®les lin√©aires g√©n√©ralis√©s
### Les mod√®les non-lin√©aires
## Mod√®les √† effets mixtes
### Mod√®les mixtes non-lin√©aires
## Aller plus loin
### Statistiques g√©n√©rales
### Statistiques avec R

<!--chapter:end:07_modelstats.Rmd-->


# Association {#chapitre-association}

Placeholder


## Espaces d'analyse
### Abondance et occurence
### Environnement
## Analyse d'association
### Association entre objets (mode Q)
#### Objets: Abondance
#### Objets: Occurence (pr√©sence-absence)
#### Objets: Donn√©es quantitatives
#### Objets: Donn√©es mixtes
### Associations entre variables (mode R)
#### Variables: Abondance
#### Variables: Occurence
#### Variables: Quantit√©s
### Conclusion sur les associations

<!--chapter:end:08_association.Rmd-->

---
title: "√âcologie num√©rique: Ordination"
author: "Serge-√âtienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Analyse statistique multivari√©e {#chapitre-anastatmv}

## Introduction

Les m√©thodes multivari√©es constituent un ensemble d‚Äôoutils statistiques permettant aux utilisateurs de tirer le maximum d‚Äôinformation contenu dans les tableaux √† plusieurs variables. Dans l‚Äô√©tude avec plusieurs unit√©s d‚Äô√©chantillonnage et plusieurs variables (ex. abondance de plusieurs esp√®ces dans plusieurs placeaux, plusieurs descripteurs quantitatifs et/ou qualitatifs mesur√©s sur plusieurs graines/fruits/arbres, etc.), l‚Äôapport des m√©thodes statistiques multivari√©es est d√©terminant. Elles visent √† structurer et simplifier les donn√©es issues de plusieurs variables, sans privil√©gier l'une d'entre elles en particulier.

Le choix d‚Äôune m√©thode d√©pend de l‚Äôobjectif initial, des types de variables manipul√©es mais aussi, de la forme des r√©sultats √† obtenir. Il existe diff√©rents groupes de m√©thodes multivari√©es. Nous pr√©sentons ici, les m√©thodes d‚Äôordination (ce chapitre) et les m√©thodes de classification (chapitre suivant) qui font partie des m√©thodes multivari√©es couramment utilis√©es.

En **√©cologie**, **biologie**, **agronommie** comme en **foresterie**, la plupart des tableaux de donn√©es comprennent de nombreuses variables : pH, teneurs en nutriments (N, P, K, Mg, Ca, ...), variables du climat (pluviom√©trie, temp√©rature, SDI, ...), esp√®ces ou cultivars et leurs param√®tres v√©g√©tatifs et de rendements, topographie, etc. 

L'**ordination** vise √† mettre de l'ordre dans de telles donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d'appr√©ciation et d'interpr√©taion ([**Legendre et Legendre, 2012**](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). 

<div class="alert alert-block alert-success">**üíæ** Explicitement, le terme __ordination__ est utilis√© en √©cologie pour d√©signer les techniques de r√©duction d'axe. Ces techniques permettent de d√©gager l'information la plus importante en projetant une synth√®se des relations entre les **observations** et entre les **variables**. Certaines techniques ne supposant aucune structure *a priori* sont dites **non-contraignantes** : elles ne comprennent pas de tests statistiques. √Ä l'inverse, les **ordinations contraignantes** lient des variables descriptives avec une ou plusieurs variables pr√©dictives. La r√©f√©rence en la mati√®re est indiscutablement le livre **"Numerical Ecology"** de [**Legendre et Legendre  (2012)**](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).</div>

L'**analyse en composantes principales** est probablement la plus connue de ces techniques. Mais de nombreuses autres techniques ont √©t√© d√©velopp√©es au cours des derni√®res ann√©es, chacune ayant ses domaines d'application. Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropri√©e pour vos donn√©es.

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous serez en mesure d'effectuer des calculs d'__ordination__ √† l'aide des techniques communes de __r√©duction d'axes__ entre autres :

- l'Analyse en Composantes Principales (ACP) - Principal Components Analysis (PCA), 
- l'Analyse de Correspondance (AC) - Correspondence Analysis (CA), encore AFC avec variantes AFCS, AFCM ?
- l'Analyse en Coordonn√©es Principales (ACoP) - Principal Coordinates Analysis (PCoA), variante du PoMd,
- l'Analyse Discriminante Lin√©aire (ADL) - Linear Discriminant Analysis (LDA), 
- l'Analyse de Redondance (RDA) - Redundancy Analysis (RDA), et 
- l'Analyse canonique des correspondances (ACC) - Canonical Correspondence Analysis (CCA), variante AC sous contrainte ?

***

## Ordination non contraignante

Cette section couvrira :

- l'**analyse en composantes principales** (ACP), 
- l'**analyse de correspondance** (AC), 
- l'**analyse factorielle** (AF), ainsi que 
- l'**analyse en coordonn√©es principales** (ACoP).

| M√©thode | Distance pr√©serv√©e | Variables (type de donn√©es) |
|---|---|---|
| Analyse en composantes principales (ACP) | Distance euclidienne | Donn√©es quantitatives, relations lin√©aires (attention aux double-z√©ros) |
| Analyse de correspondance (AC) | Distance de $\chi^2$ | Donn√©es non-n√©gatives, dimensionnellement homog√®nes ou binaires, abondance ou occurence |
| Positionnement multidimensionnel (PoMd) | Toute mesure de dissimilarit√© | Donn√©es quantitatives, qualitatives nominales/ordinales ou mixtes |

Source: Adapt√© de ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0), chapitre 9)

### Analyse en composantes principales

#### Description

L'objectif d'une ACP est de repr√©senter les donn√©es dans un nombre r√©duit de dimensions repr√©sentant le plus possible la variation d'un tableau de donn√©es : elle permet de projetter les donn√©es dans un espace o√π les variables sont combin√©es en axes orthogonaux dont le premier axe capte le maximum de variance. L'ACP peut par exemple √™tre utilis√©e pour analyser des corr√©lations entre variables ou d√©gager l'information la plus pertinente d'un tableau de donn√©es m√©t√©o ou de signal en un nombre plus retreint de variables.

<div class="alert alert-block alert-success">**üíæ** L'ACP effectue une rotation des axes √† partir du centre (moyenne) du nuage de points effectu√©e de mani√®re √† ce que le premier axe d√©finisse la direction o√π l'on retrouve la variance maximale. Ce premier axe est une combinaison lin√©aire des variables et forme la premi√®re composante principale. Une fois cet axe d√©finit, on trouve le deuxi√®me axe, orthogonal au premier, o√π l'on retouve la variance maximale - cet axe forme la deuxi√®me composante principale, et ainsi de suite jusqu'√† ce que le nombre d'axe corresponde au nombre de variables.</div> 

Conceptuellement, toutes les colonnes d'un jeu de donn√©es contiennent de l'information potentiellement interessante. L'ACP cr√©e un jeu de donn√©es artificiel avec un nombre de dimensions √©gal √† celui du premier. La seule diff√©rence est que ses premi√®res dimensions concentrent la majeure partie de l'information. Dans le monde de l'ACP, l'information est appel√©e inertie. Les dimensions sont appel√©es facteurs ou axes principaux.

  - Les projections des **observations** sur ces axes principaux sont appel√©s les **scores** ou **valeurs propres** (*eigenvalues*). 

  - Les projections des **variables** sur les axes principaux sont les **vecteurs propres**  (*eigenvectors*, ou *loadings*). 

  - La variance des composantes principales diminue de la premi√®re √† la derni√®re, et peut √™tre calcul√©e comme une proportion de la variance totale : c'est le **pourcentage d'inertie**. 

  - Par convention, on utilise les **valeurs propres** (*eigenvalues*) pour mesurer l'importance des axes. 

  - Si la premi√®re composante principale a une inertie de 50% et la deuxi√®me une intertie de 30%, la repr√©sentation en $2D$ des projections repr√©sentera 80% de la variance du nuage de points. L'information perdue est donc de 20% sur les dimensions initiales r√©duites √† 2 dimensions.

L'**h√©t√©rog√©n√©it√© des √©chelles de mesure** peut avoir une grande importance sur les r√©sultats d'une ACP (_les donn√©es doivent √™tre dimensionnellement homog√®nes_). En effet, la hauteur d'un *...c√©riser...* aura une variance plus grande que le diam√®tre d'une *...c√©rise...* exprim√© dans les m√™mes unit√©s, et cette derni√®re aura plus de variance que la teneur en cuivre d'une feuille. 

  - Il est cons√©quemment avis√© de mettre les donn√©es √† l'√©chelle en centrant la moyenne √† $0$ et l'√©cart-type √† $1$ avant de proc√©der √† une ACP. L'ACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les **distributions** sont **multinormales**. Bien que l'ACP soit une technique robuste, il est pr√©f√©rable de transformer pr√©alablement les variables dont la distribution est particuli√®rement asym√©triques ([Legendre et Legendre, 2012, p. 450](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). 
  
Le cas √©ch√©ant, les valeurs extr√™mes pourraient faire d√©vier les vecteurs propres et biaiser l'analyse. En particulier, les ACP men√©es sur des donn√©es compositionnelles sont r√©put√©es pour g√©n√©rer des analyses biais√©es ([Pawlowsky-Glahn and Egozcue, 2006](http://sp.lyellcollection.org/content/specpubgsl/264/1/1.full.pdf)). 

<div class="alert alert-block alert-success">**üíæ** Le **test de Mardia** ([Korkmaz, 2014](https://journal.r-project.org/archive/2014-2/korkmaz-goksuluk-zararsiz.pdf)) peut √™tre utilis√© pour tester la multinormalit√©.</div> 

  - Une distribution multinormale devrait g√©n√©rer des scores en forme d'hypersph√®re (en forme de cercle sur un biplot : voir plus loin).

#### Vecteurs propres et valeurs propres

<div class="alert alert-block alert-success">**üíæ** L'algorithme de l'**ACP** effectue sur la matrice individus/variables, diff√©rentes op√©rations, 

- centrage-r√©duction des donn√©es, 
- diagonalisation de la matrice de corr√©lation, 
- extraction de valeurs propres et de vecteurs propres, 
- etc.

en vue de passer du nombre de variables initiales √† un nombre r√©duit de variables obtenues par combinaison lin√©aire des premi√®res :les composantes principales ([Kakai et al., 2016](https://www.researchgate.net/publication/301214230_Methodes_statistiques_multivariees_utilisees_en_ecologie)).</div>

Une matrice carr√©e (comme une matrice de covariance, appel√©e $\Sigma$) multipli√©e par un vecteur propre ($e$) est √©gale aux valeurs propres ($\lambda$) multipli√©es par les vecteurs propres ($e$).

$$ \Sigma e = \lambda e $$

De mani√®re intuitive,

  - les **vecteurs propres** indiquent l'orientation de la covariance, et 
  - les **valeurs propres** indiquent la longueur associ√©e √† cette direction. 
  - L'ACP est bas√©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. 

Pour d'abord obtenir les valeurs propres ($\lambda$), il faut r√©soudre l'√©quation

$$ det(cov(X) - \lambda I) = 0 $$ 

o√π 

  - $det()$ est l'op√©ration permettant de calculer le d√©terminant, 
  - $cov()$ est l'op√©ration pour calculer la covariance, 
  - $X$ est la matrice de donn√©es (le dataframe), 
  - les $\lambda$ sont les valeurs propres et 
  - $I$ est une matrice d'identit√©.

Pour $p$ variables dans votre tableau $X$, vous obtiendrez $p$ valeurs propres. Ensuite, on trouve les vecteurs propres en r√©solvant l'√©quation $ \Sigma e = \lambda e $.

Bien qu'il soit possible d'[effectuer cette op√©ration √† la main](https://www.youtube.com/watch?v=2fCBE7DWgd0&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM) pour des cas tr√®s simples, vous aurez avantage √† utiliser un langage de programmation.

Chargeons les donn√©es d'iris, puis isolons seulement les deux dimensions des s√©pales de l'esp√®ce *setosa*.

```{r}
library("tidyverse")
data("iris")
setosa_sepal <- iris %>% 
  filter(Species == "setosa") %>% 
  select(starts_with("Sepal")) %>% 
  rename(
    long_sepal = Sepal.Length,
    larg_sepal = Sepal.Width)
setosa_sepal
```

Test de multinormalit√© de Mardia :

```{r}
library("MVN")
setosa_sepal_mvn <- mvn(setosa_sepal, mvnTest = "mardia")
setosa_sepal_mvn$multivariateNormality
```

**H0 du test** : La distribution n'est pas multinormale.

<div class="alert alert-block alert-success">**üíæ** Pour consid√©rer la distribution comme multinormale, les **p-value** de la distortion (`Mardia Skewness`) **et** de la statistique de Kurtosis (`Mardia Kurtosis`) doivent √™tre √©gales ou plus √©lev√©es que $0.05$ ([Kormaz, 2019, fiche d'aide de la fonction `mvn` de R](https://www.rdocumentation.org/packages/MVN/versions/5.6/topics/mvn)).</div>

C'est bien le cas pour les donn√©es du tableau `setosa_sepal`.

Pour des petits jeux de donn√©es, on pourra faire certains calculs √† la main. L'id√©e majeure dans ce cas est de savoir techniquement comment √ßa se fait. Pour rappels les calculs √† la mitaine :

  - de la moyenne, 

$$
\bar{x} = \frac{1}{N} \sum_{i=1}^{n} n_{i} x_{i}
$$

  - de la variance, 

$$
Var(X) =\frac{1}{N} \sum_{i=1}^{n} n_{i}\left(\bar{x}-x_{i}\right)^{2}
$$

  - de l'√©cart-type, 

$$
\sigma_{X}=\sqrt{\operatorname{Var}(x)}
$$

  - de la covariance, avec une 2nde formule de calcule plus pratique 

$$
\operatorname{Cov}(X, Y)=\frac{1}{N} \sum\left(x_{i}-\mu(X)\right) \times\left(y_{i}-\mu(Y)\right)
$$

$$
\operatorname{Cov}(X, Y)=\left(\frac{1}{N} \sum x_{i} y_{i}\right)-\mu(X) \times \mu(Y)
$$

  - du coefficient de corr√©lation

$$
\operatorname{Corr}(X, Y)=\frac{\operatorname{Cov}(X, Y)}{\sigma(X) \sigma(Y)}
$$

Mais profitons du g√©nie logiciel avec R. Retirons de la matrice de covariance, les valeurs et vecteurs propres avec la fonction `eigen` du package `base`.

```{r}
setosa_eigen <- eigen(cov(setosa_sepal))

setosa_eigenval <- setosa_eigen$values
setosa_eigenval
```

```{r}
setosa_eigenvec <- setosa_eigen$vectors
setosa_eigenvec
```

Le premier vecteur propre correspond √† la premi√®re colonne, et le second √† la deuxi√®me. Les coordonn√©es $x$ et $y$ sont les premi√®res et deuxi√®mes lignes.

Les **vecteurs propres** ont une longueur unitaire (norme de 1). Ils peuvent √™tre mis √† l'√©chelle √† la racine carr√©e des valeurs propres.

```{r}
setosa_eigenvec_sc <- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values))
setosa_eigenvec_sc
```

Un aper√ßu de la racine carr√©e des valeurs propres en matrice diagonale :

```{r}
diag(sqrt(setosa_eigen$values))
```

Pour effectuer la translation des vecteurs propres au centre du nuage de point, nous avons besoin du centro√Øde.

```{r}
centroid <- setosa_sepal %>% apply(., 2, mean)
centroid
```

Aper√ßu graphique :

  - Le nuage de points

```{r}
plot(setosa_sepal, asp = 1) # asp, the y/x aspect ratio
```

  - Ajout des vecteurs propres brutes en couleur verte :
  
```{r}
plot(setosa_sepal, asp = 1) # asp, the y/x aspect ratio
# vecteurs propres brutes
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = "green", lwd = 3) # vecteur propre 1
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = "green", lwd = 3) # vecteur propre 2
```

  - Ajout des vecteurs propres √† l'√©chelle en couleur rouge :

```{r}
plot(setosa_sepal, asp = 1) # asp, the y/x aspect ratio
# vecteurs propres brutes
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), 
      col = "green", lwd = 3) # vecteur propre 1
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), 
      col = "green", lwd = 3) # vecteur propre 2
# vecteurs propres √† l'√©chelle
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), 
      col = "red", lwd = 4) # vecteur propre 1
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), 
      col = "red", lwd = 4) # vecteur propre 2
```

  - Ajout du centroid, le point bleu :

```{r}
plot(setosa_sepal, asp = 1) # asp, the y/x aspect ratio
# vecteurs propres brutes
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), 
      col = "green", lwd = 3) # vecteur propre 1
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), 
      col = "green", lwd = 3) # vecteur propre 2
# vecteurs propres √† l'√©chelle
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), 
      col = "red", lwd = 4) # vecteur propre 1
lines(x = c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]),
      y = c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), 
      col = "red", lwd = 4) # vecteur propre 2

points(x = centroid[1], y = centroid[2], pch = 16, cex = 2, col = "blue") # centroid
```

On peut observer que, comme mentionn√© plus haut, **les vecteurs propres indiquent l'orientation de la covariance, et les valeurs propres indiquent la longueur associ√©e √† cette direction**.

#### Biplot de l'ACP

Imaginez un nuage de points en $3D$, axe $y$ compris. 

![](images/iris-3d.png)

Vous tournez votre nuage de points pour trouver la perspective en $2D$ qui fera en sorte que vos donn√©es soient les plus dispers√©es possibles, voir pr√©sentation grossi√®re :

![](images/nuage-de-points-3d.gif)

[Nuage de points en 3 dimensions](https://www.google.com/url?sa=i&url=https%3A%2F%2Fnumerisation3d.construction%2Fnuages-de-points-meshs%2F&psig=AOvVaw1k5OLYTl_sNuvM5HSuuWio&ust=1620994315147000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCOjkz8rQxvACFQAAAAAdAAAAABAE)

Lorsque la perspective est trouv√©e, avec une lampe de poche, vous illuminez votre nuage de points dans l'axe de cette perspective : vous venez d'effectuer une analyse en composantes principales, et **l'ombre des points et des axes sur le mur formera votre biplot**.

Pour cr√©er un biplot, on juxtapose :

  - les **descripteurs** (les variables) en tant que vecteurs propres, repr√©sent√©s par des fl√®ches,

  - et les **objets** (les observations) en tant que scores (valeurs propres), repr√©sent√©s par des points. 

Les r√©sultats d'une ordination peuvent √™tre pr√©sent√©s selon deux types de biplots ou de projections ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)) courramment utilis√©s :

<div class="alert alert-block alert-success">**üíæ** **Les Biplots de distance**

Ce type de projection permet de visualiser la position des **objets** (observations) entre eux et par rapport aux **descripteurs** (variables) et d'appr√©cier la contribution des descripteurs pour cr√©er les composantes principales.</div> 

Pour cr√©er un biplot de distance, on projette directement les vecteurs propres ($U$) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de l'ACP ($F$). De cette mani√®re, 

1. les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans l'espace multidimentionnel,

2. la projection d'un objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l'objet sur le descripteur et

3. la projection d'un descripteur sur un axe principal est proportionnelle √† sa contribution pour g√©n√©rer l'axe.

<div class="alert alert-block alert-success">**üíæ** **Les Biplots de corr√©lation** 

Cette projection permet d'appr√©cier les corr√©lations entre les descripteurs.</div> 

Pour ce faire, 

  - les objets et les valeurs propres doivent √™tre transform√©s.

  - Pour g√©n√©rer les descripteurs, les vecteurs propres ($U$) doivent √™tre multipli√©s par la matrice diagonalis√©e de la racine carr√©e des valeurs propres ($\Lambda$), c'est-√†-dire $U \Lambda ^{\frac{1}{2}}$. 

  - En ce qui a trait aux objets, on multiplie les scores ($F$) par la racine carr√©e n√©gative des valeurs propres diagonalis√©es, c'est-√†-dire $F \Lambda ^{- \frac{1}{2}}$. 

De cette mani√®re, 

1. tout comme c'est le cas pour le biplot de distance, la projection d'un objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l'objet sur le descripteur,

2. la projection d'un descripteur sur un axe principal est proportionnelle √† son √©cart-type et

3. les **angles** entre les descripteurs sont proportionnels √† leur corr√©lation (et non pas leur proximit√©).

<div class="alert alert-block alert-success">**üíæ** En d'autres mots, le biplot de distances devrait √™tre utilis√© pour appr√©cier la distance entre les objets et le biplot de corr√©lation devrait √™tre utilis√© pour appr√©cier les corr√©lations entre les descripteurs.</div>

Mais dans tous les cas, **le type de biplot utilis√© doit √™tre indiqu√©**.

![](images/10_bipolot-meteo-sc2.svg)

<center>**Biplot de corr√©lation** permettant de visualiser les corr√©lations entre des variables m√©t√©orologiques. Source: [Parent, 2017]()</center>

<div class="alert alert-block alert-success">**üíæ** Le *triplot* est une forme apparent√©e au **biplot**, auquel on ajoute des variables pr√©dictives. Le triplot est utile pour repr√©senter les r√©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques.</div>

#### Application

Bien que l'ACP puisse √™tre effectu√©e gr√¢ce √† des modules de base de R, nous utiliserons le module `vegan`. Le tableau [`varechem`](https://rdrr.io/rforge/vegan/man/varechem.html) comprend des donn√©es issues d'analyse de sols identifi√©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de l'humus publi√©es dans [V√§re et al. (1995)](http://onlinelibrary.wiley.com/doi/10.2307/3236351/abstract) et export√©es du module [vegan](https://rdrr.io/rforge/vegan/).

```{r}
library("vegan")
data("varechem")
varechem %>% 
  sample_n(5)
```

Comme nous l'avons vu pr√©cdemment, les donn√©es de concentration sont de type *compositionnelles*. Les donn√©es compositionnelles du tableau `varechem` m√©riteraient d'√™tre transform√©es ([Aitchison et Greenacre, 2002](http://doi.wiley.com/10.1111/1467-9876.00275)). Utilisons les log-ratios centr√©s (*clr*).

```{r}
library("compositions")

varecomp <- varechem %>%
  select(-Baresoil, -Humdepth, -pH) %>% 
  mutate(Fv = apply(., 1, function(x) 1e6 - sum(x)))

vareclr <- varecomp %>%
  acomp(.) %>%
  clr(.) %>% 
  as_tibble() %>% 
  bind_cols(varechem %>%
              select(Baresoil, Humdepth, pH))
vareclr %>% 
  sample_n(5)
```

Effectuons l'**ACP**. Pour cet exemple, nous standardiserons les donn√©es √©tant donn√©es que les colonnes `Baresoil`, `Humedepth` et `pH` ne sont pas √† la m√™me √©chelle que les colonnes des **clr**.

```{r}
vareclr_sc <- scale(vareclr)
vare_pca <- rda(vareclr_sc) 
# ou bien rda(vareclr, scale = TRUE, mais la mise √† l'√©chelle pr√©alable est plus explicite)
```

L'objet `vareclr_pca` contient l'information n√©cessaire pour mener notre ACP.

```{r}
summary(vare_pca, scaling = 2) 
# scaling = 2 pour obtenir les infos pour les biplots de corr√©lation
```

La deuxi√®me ligne de `Importance of components`, `Proportion Explained`, indique la proportion de la variance totale capt√©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxi√®me axe principal ajoutant une proportion de 16,51%, une repr√©sentation en deux axes principaux pr√©sentent 64.19 % de la variance.

```{r}
prop_expl <- vare_pca$CA$eig / sum(vare_pca$CA$eig)
prop_expl
```

<div class="alert alert-block alert-success">**üíæ** La d√©cision du nombre d'axes principaux √† retenir est arbitraire.</div>

  - Elle peut d√©pendre d'un nombre maximal de param√®tres √† retenir pour √©viter de surdimensionner un mod√®le (*curse of dimensionality*, section 11),

  - ou d'un seuil de pourcentage de variance minimale √† retenir, par exemple 75%,

  - ou bien, vous retiendrez deux composantes principales si vous d√©sirez pr√©senter un seul biplot. 

<div class="alert alert-block alert-success">**üíæ** L'approche de *Kaiser-Guttmann* ([Borcard et al., 2011](http://www.springer.com/us/book/9781441979759)) consiste √† s√©lectionner les composantes principales dont la valeur propre est sup√©rieure √† leur moyenne.</div>

```{r}
plot(x = 1:length(vare_pca$CA$eig),
     y = vare_pca$CA$eig,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
abline(h = mean(vare_pca$CA$eig), col = "red", lty = 2)
```

<div class="alert alert-block alert-success">**üíæ** L'approche du *broken stick* consiste √† couper un b√¢ton d'une longueur de 1 en n tranches.</div>

  - La premi√®re tranche est de longueur $\frac{1}{n}$. 

  - La tranche suivante est d'une longueur de la tranche pr√©c√©dente √† laquelle on aditionne  $\frac{1}{longueur~restante}$. 

  - Puis on place les longueurs en ordre d√©croissant. 

  - On retient les composantes principales dont les valeurs propres cumul√©es sont plus grandes que le broken stick.

```{r}
broken_stick <- function(x) {
  bsm <- vector("numeric", length = x) # broken-stick method bsm, cr√©√©.
  bsm[1] <- 1/x
  for (i in 2:x) {
    bsm[i] <- bsm[i-1] + 1/(x+1-i)
  }
  bsm <- rev(bsm/x)
  return(bsm)
}

```

Le graphique du *broken stick* :

```{r}
plot(x = 1:length(vare_pca$CA$eig),
     y = prop_expl,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
lines(x = 1:length(vare_pca$CA$eig),
      y = broken_stick(length(vare_pca$CA$eig)),
      col = "red",
      lty = 2)
```

Les approches *Kaiser-Guttmann* et *broken stick* sugg√®rent que les trois premi√®res composantes sont suffisantes pour d√©crire la dispersion des donn√©es.

<div class="alert alert-block alert-success">**üíæ** G√©n√©ralement, on estime qu‚Äôune concentration d‚Äôinformation de 50 % du tableau de d√©part est suffisante pour garantir une pr√©cision d‚Äôanalyse et sert de crit√®re de choix du nombre de composantes principales √† retenir ([Kakai et al., 2016](https://www.researchgate.net/publication/301214230_Methodes_statistiques_multivariees_utilisees_en_ecologie)).</div>

Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module `vegan`, les vecteurs propres sont les esp√®ces (`species`) et les scores sont les `sites`.

```{r}
vare_eigenvec <- vegan::scores(vare_pca, scaling = 2, display = "species", 
                               choices = 1:(ncol(vareclr)-1))
vare_eigenvec
```

- L'ordre d'importance des vecteurs propres est √©tabli en ordre croissant des √©l√©ment des vecteurs propres associ√©es. 

- Un vecteur propre est une combinaison lin√©aire des variables. Par exemple, 

  - le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de l'Al (-1.463). 
  - Le deuxi√®me pointe surtout vers le Mo (2.145). 

- Les vecteurs (*loadings*) d'un biplot de distance pr√©sentant les deux premi√®res composantes principales prendront les coordonn√©es des deux premi√®res colonnes.

  - Le vecteur **Al** aura la coordonn√©e [-1.463 ; -0.601], 
  - le vecteur de **Fe** sera plac√© √† [-1.497 ; -0.606] 
  - et le vecteur **Mo** √† [-0.312 ; 2.145]. 

Il existe diff√©rentes fonctions d'affichage des biplots. Notez que leur longueur peut √™tre magnifi√©e pour am√©liorer la visualisation.

Lan√ßons la fonction `biplot` pour cr√©er un biplot de distance et un autre de corr√©lation.

```{r}
par(mfrow = c(1, 2))
biplot(vare_pca, scaling = 1, main = "Biplot de distance")
biplot(vare_pca, scaling = 2, main = "Biplot de corr√©lation")
```

Le biplot de distance permet de d√©gager les variables qui expliquent davantage la variabilit√© dans notre tableau : 

  - les clr du **Fe** et de l'**Al** forment en grande partie le premier axe principal, alors que le clr du **Mo** forme en grande partie le second axe. 

Le biplot de corr√©lation montre que les clr du **Fe** et du **Al** sont corr√©l√©s dans le m√™me sens, mais dans le sens contraire du clr du **Mn**. L'information sur la teneur en **Fe** et celle de l'**Al** est en grande partie redondante. Toutefois, le clr du **Mo** est presque ind√©pendant du clr du **Fe**, ceux-ci √©tant √† angle presque droit (~90¬∞). Ces relations peuvent √™tre explor√©es directement.

```{r}
par(mfrow = c(1, 2))
plot(vareclr$Al, vareclr$Fe)
plot(vareclr$Mo, vareclr$Fe)
```

Nous avons mentionn√© que l'**ACP** est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donn√©es que nous chargerons provient d'une infographie d'un dauphin, intitull√©e *Bottlenose Dolphin*, con√ßu par l'artiste [Tarnyloo](https://www.blendswap.com/blends/view/83681). Les points correspondent √† la surface d'un dauphin. J'ai ajout√© une colonne `anatomy`, qui indique √† quelle partie anatomique le point appartient.

```{r}
dolphin <- read_csv("data/dolphin.csv")
dolphin %>% sample_n(5)
```

Voici en vue isom√©trique ce en quoi consiste ce nuage de points.

```{r}
library("scatterplot3d")
scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2)
```

Effectuons l'**ACP** sur le dauphin.

```{r}
dolph_pca <- rda(dolphin %>% select(x, y, z), scale = FALSE)
biplot(dolph_pca, scaling = 2)
```

On n'y voit pas grand chose, mais si l'on extrait les scores et que l'on raccourcit les vecteurs :

```{r}
dolph_scores <- vegan::scores(dolph_pca, display = "sites")
dolph_loads <- vegan::scores(dolph_pca, display = "species")
dolph_loads

plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy))
segments(x0 = rep(0, 3), y0 = rep(0, 3),
         x = dolph_loads[, 1]/50,
         y = dolph_loads[, 2]/50,
         col = "chocolate", lwd = 4)
```

La meilleure repr√©sentation du dauphin en $2D$, selon la variance, est son profil - en effet, il est plus long et haut que large.

<div class="alert alert-block alert-info">**Note**. Une **ACP** effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues.</div>

<div class="alert alert-block alert-info">**Note**. L'**ACP** a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce n'est √©videmment pas le cas du dauphin).</div>

<div class="alert alert-block alert-info">**Note**. Les axes principaux d'une **ACP** sont des variables al√©atoires. Elles peuvent √™tre assujetties √† des tests ststistiques, des mod√®les, du partitionnement de donn√©es, etc.</div>

<div class="alert alert-block alert-success">**Excercice**. Effectuez maintenant une **ACP** avec les donn√©es d'iris.</div>

### Analyse de correspondance (AC), encore AFC ?

#### Description

Le but de la m√©thode est aussi de r√©sumer l‚Äôinformation contenue dans un tableau √† plusieurs variables, mais en d√©crivant les relations entre les √©l√©ments-lignes (individus) et les √©l√©ments-colonnes (variables). Ainsi, l‚Äô**AFC** est une sorte de double **ACP** en ce sens qu‚Äôelle s‚Äôint√©resse √† la fois aux lignes et aux colonnes du tableau. 

<div class="alert alert-block alert-success">Le tableau sur lequel s‚Äôapplique l‚Äô**AFC** est un tableau de contingence, c‚Äôest-√†-dire un tableau √† double entr√©e avec des valeurs de comptage ou de fr√©quence absolues (**donn√©es d'abondance et d'occurence**) dans les cellules (Gl√®l√® Kakai et al., 2016).</div>

Tout comme l'**ACP**, les donn√©es apport√©es vers une **AC** doivent √™tre dimensionnellement homog√®nes, c'est-√†-dire que chaque variable doit √™tre de m√™me m√©trique :

<div class="alert alert-block alert-success">**üíæ** Pour des donn√©es d'abondance, cela signifie que les d√©comptes r√©f√®rent tous au m√™me concept : individus, colonies, surfaces occup√©es, etc.</div>

<div class="alert alert-block alert-success">**üíæ** Alors que la distance euclidienne est pr√©serv√©e avec l'**ACP**, l'**AC** pr√©serve la distance du $\chi^2$, qui est insensible aux double-z√©ros.</div>

<div class="alert alert-block alert-success">**üíæ** L'**AC** produit $min(n,p)-1$ axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carr√© par rapport √† la somme des carr√©s de la matrice.</div>

Le biplot obtenu peut √™tre pr√©sent√© sous forme :

- de biplot de site `scaling 1`, o√π la distance du $\chi^2$ est pr√©serv√©e entre les sites 

- ou de biplot d'esp√®ces `scaling 2`, o√π la distance du $\chi^2$ est pr√©serv√©e entre les esp√®ces. 

<div class="alert alert-block alert-success">**üíæ** L'**AC** h√©rite du coup une propri√©t√© importante de la distance du $\chi^2$, qui accorde davantage de distance entre un compte de 0 et de 1 qu'entre 1 et 2, et davantage entre 1 et 2 qu'entre 2 et 3.</div>

Par exemple, sur ces trois sites, on a compt√© un individu **A** de moins que les individus **B**.

```{r}
abundance_0123 = tibble(Site = c("Site 1", "Site 2", "Site 3"),
                        A = c(0, 1, 9),
                        B = c(1, 2, 10))
abundance_0123
```

Pourtant, la distance du $\chi^2$ est plus √©lev√©e entre le site 1 et le site 2 qu'entre le site 2 et le site 3.

```{r}
dist(decostand(abundance_0123 %>% 
                 select(-Site), method = "chi.square"))
```

<div class="alert alert-block alert-success">**üíæ** La distance du $\chi^2$ donne davantage d'importance aux esp√®ces rares, ce dont une analyse doit tenir compte. Il pourrait √™tre envisageable de retirer d'un tableau des esp√®ces rares, ou bien pr√©transformer des donn√©es d'abondance par une transformation de **Chord** ou de **Hellinger** (tel que discut√© au chapitre ...), puis proc√©der √† une **ACP** sur ces donn√©es ([**Legendre et Gallagher, 2001**](https://doi.org/10.1007/s004420100716)).</div>

#### Application

Le tableau [`varespec`](https://rdrr.io/rforge/vegan/man/varechem.html) comprend des donn√©es de surface de couverture de 44 esp√®ces de plantes en lien avec les donn√©es environnementales du tableau `varechem`. Ces donn√©es ont √©t√© publi√©es dans [V√§re et al. (1995)](http://onlinelibrary.wiley.com/doi/10.2307/3236351/abstract) et export√©es du module [vegan](https://rdrr.io/rforge/vegan/).

```{r}
data("varespec")
varespec %>% sample_n(5)
```

Pour effectuer l'**AC**, nous utiliserons, comme pour l'**ACP**, le module `vegan` mais cette fois-ci avec la fonction `cca()`. 

- L'**AC** en `scaling 1` est effectu√©e sur le tableau des abondances avec les **esp√®ces** comme colonnes et les **sites** comme lignes. 

- Les matrices d'abondance transpos√©es indiquent les sites o√π chaque esp√®ce a √©t√© d√©nombr√©e : pour une analyse en `scaling 2`, on effectue une analyse de correspondance sur la matrice d'abondance (ou d'occurence) transpos√©e.

Pour chacune des **AC**, je filtre pour m'assurer que toutes les lignes contiennent au moins une observation. Ce n'est pas n√©cessaire dans notre cas, mais je le laisse pour l'exemple.

```{r}
vare_cca <- cca(varespec %>% filter(rowSums(.) > 0))
summary(vare_cca, scaling = 1)
```

```{r}
varespec_eigenval <- eigenvals(vare_cca, scaling = 1)
prop_expl <- varespec_eigenval / sum(varespec_eigenval)
prop_expl
```

```{r}
par(mfrow = c(1, 2))
plot(x = 1:length(varespec_eigenval),
     y = vare_cca$CA$eig,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
abline(h = mean(varespec_eigenval), col = "red", lty = 2)

plot(x = 1:length(varespec_eigenval),
     y = prop_expl,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Proportion de la valeur propre")
lines(x = 1:length(varespec_eigenval),
      y = broken_stick(length(varespec_eigenval)),
      col = "red",
      lty = 2)
```

Cr√©ons les biplots.

```{r, fig.height = 5, fig.width = 10}
par(mfrow = c(1, 2))
plot(vare_cca, scaling = 1, main = "Biplot des esp√®ces")
plot(vare_cca, scaling = 2, main = "Biplot des sites")
```

<div class="alert alert-block alert-success">**üíæ** Le **biplot des esp√®ces**, √† gauche (`scaling = 1`), montre la distribution des sites selon les esp√®ces. Les emplacements des scores (en noir) montrent les contrastes entre les sites selon les esp√®ces qui les recouvrent.</div>

Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2i√®me axe principal. Par ailleurs, les axes principaux sont form√© de plusieurs esp√®ces dont aucune ne domine clairement. ....√† corriger....

<div class="alert alert-block alert-success">**üíæ** Le **biplot des sites**, √† droite (`scaling = 2`), montre la distribution des recouvrements d'esp√®ces selon les sites.</div>

Par exemple, les esp√®ces Betupube ([*Betula pubescens*](https://fr.wikipedia.org/wiki/Betula_pubescens)) et Barbhatc ([*Barbilophozia hatcheri *](https://en.wikipedia.org/wiki/Marchantiophyta)) se recouvrent en particulier sur le site 24. ....corriger....

Le site 1 est difficile √† identifier, car il est couvert par plusieurs noms d'esp√®ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une esp√®ce de [*Dicranum*](https://en.wikipedia.org/wiki/Dicranum)) qui le recouvre amplement. ...corriger...

Pour les deux types de biplot, les sites o√π les esp√®ces situ√©s pr√®s de l'origine montre qu'ils peuvent √™tre soit pr√®s de la moyenne, soit distribu√©s uniform√©ment.

<div class="alert alert-block alert-success">**üíæ** Le nombre de composantes √† retenir peut aussi √™tre √©valu√© par les approches *Kaiser-Guttmann* et *broken-stick*.</div>

```{r, fig.width=10, fig.height=4}
scaling <- 1
varespec_eigenval <- eigenvals(vare_cca, scaling = scaling) # peut √™tre effectu√© sur les deux types de scaling

prop_expl <- varespec_eigenval / sum(varespec_eigenval)

par(mfrow = c(1, 2))
plot(x = 1:length(varespec_eigenval),
     y = vare_cca$CA$eig,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre",
     main = paste("Eigenvalue - Kaiser-Guttmann, scaling =", scaling))
abline(h = mean(varespec_eigenval), col = "red", lty = 2)

plot(x = 1:length(varespec_eigenval),
     y = prop_expl,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Proportion",
     main = paste("Proportion - broken stick, scaling =", scaling))
lines(x = 1:length(varespec_eigenval),
      y = broken_stick(length(varespec_eigenval)),
      col = "red",
      lty = 2)
```

Pour les deux scalings, l'approche *Kaiser-Guttmann* propose 7 axes, tandis que l'approche *broken-stick* en propose 5.

<div class="alert alert-block alert-success">**üíæ** Les repr√©sentations du biplot d'analyse de correspondance peuvent prendre la forme d'un boomerang, en particulier celles qui sont bas√©es sur des donn√©es d'occurence.</div>

Le tableau suivant initialement de [Chessel et al. (1987)](http://pbil.univ-lyon1.fr/R/pdf/pps047.pdf) est distribu√© dans le module `ade4`.

```{r}
library("ade4")
data("doubs")
fish <- doubs$fish
doubs_cca <- cca(fish %>% filter(rowSums(.) > 0))
plot(doubs_cca, scaling = 2)
```

Les num√©ros de sites correspondent √† la position dans une rivi√®re, 1 √©tant en amont et 30 en aval. 

 - Le premier axe discrimine l'amont et l'aval, tandis que le deuxi√®me montre deux niches en amont. 
 
Bien que l'on observe une discontinuit√© dans le cours d'eau, il y a une continuit√© dans les abondances. Cet effet peut √™tre corrig√© en retirant la tendance de l'analyse de correspondance par une *detrended correspondance analysis (AC redress√©e)*. Pour cela, il faudra utiliser la fonction `decorana()`, ce qui ne sera pas couvert ici.

<div class="alert alert-block alert-success">**üíæ** L'*analyse des correspondances multiples* (ACM) est utile pour l'ordination des donn√©es cat√©gorielles (qualitatives).</div>

Le module [*ade4*](https://larmarange.github.io/analyse-R/analyse-des-correspondances-multiples.html#acm-avec-ade4) est en mesure d'effectuer des **ACM**, mais n'est pas couvert dans ce manuel.

<div class="alert alert-block alert-success">**Excercice**. Effectuez et analysez une **AC** avec les donn√©es de recouvrement `varespec`.</div>

> C'est ce qui est fait ci-haut NON ? ...v√©rifier.

### Positionnement multidimensionnel (PoMd)

#### Description

<div class="alert alert-block alert-success">Le positionnement multidimensionnel (**PoMd**), ou [*Manifold Analysis*](http://scikit-learn.org/stable/modules/manifold.html), se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en r√©duire les dimensions.</div>

Alors que l'**ACP** conserve la distance euclidienne et que l'**AC** conserve la distance du $\chi^2$, le **PoMd** conserve l'association que vous s√©lectionnerez √† votre convenance. Le **PoMd** vise √† repr√©senter en un nombre limit√© de dimensions (souvent 2) la distance (ou dissimilarit√©) qu'ont des objets (ou des variables) les uns par rapport aux autres dans l'espace multidimensionnel.

Il existe deux types de **PoMd** :

1. Le **PoMd-m√©trique** (*metric multidimentional scaling* MMDS, parfois le *metric* est retir√©, MDS, et parfois l'on parle de *classic MDS*) vise √† repr√©senter fid√®lement la distance entre les objets ou les variables. Le PoMd-m√©trique ne devrait √™tre utilis√© que lorsque la m√©trique n'est ni euclidienne, ni de $\chi^2$ et que l'on d√©sire pr√©server les distances entre les objets. Le PoMd-m√©trique est aussi appel√© *analyse en coordonn√©es principales* (ACoP ou de l'anglais *PCoA*) .

1. Le **PoMd-non-m√©trique** (*nonmetric multidimentional scaling*, NMDS) vise quant √† lui √† repr√©senter l'ordre des distances entre les objets ou les variables. C'est une approche par rang : le PoMd-non-m√©trique vise √† repr√©senter les objets qui sont plus proches ou plus √©loign√©es les uns des autres plut√¥t que de repr√©senter leur similarit√© dans l'espace multidimentionnelle.

L'**IsoMap**, pour *isometric feature mapping*, est une extension du PoMd qui reconstruit les distances selon les points retrouv√©s dans le voisinage. Les isomaps sont en mesure d'applatir des donn√©es ayant des formes complexes.

Nous ne traitons pour l'instant que des PoMd-m√©triques (fonction `vegan::cmdscale()`) et des PoMd-non-m√©triques (fonction `vegan::metaMDS()`).

#### Application

Utilisons les donn√©es d'abondance que nous avions au tout d√©but du chapitre sur l'association. La matrice d'association de Bray-Curtis sera utilis√©e. 

```{r}
abundance <- tibble('Bruant familier' = c(1, 0, 0, 3),
                    'Citelle √† poitrine rousse' = c(1, 0, 0, 0),
                    'Colibri √† gorge rubis' = c(0, 1, 0, 0),
                    'Geai bleu' = c(3, 2, 0, 0),
                    'Bruant chanteur' = c(1, 0, 5, 2),
                    'Chardonneret' = c(0, 9, 6, 0),
                    'Bruant √† gorge blanche' = c(1, 0, 0, 0),
                    'M√©sange √† t√™te noire' = c(20, 1, 1, 0),
                    'Jaseur bor√©al' = c(66, 0, 0, 0))
```

```{r, fig.width=3, fig.height=2.4}
assoc_mat <- vegdist(abundance, method = "bray")
pheatmap::pheatmap(assoc_mat %>% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE,
         display_numbers = round(assoc_mat %>% as.matrix(), 2))
```

Les sites 2 et 3 devraient √™tre plus pr√®s l'un et l'autre, puis les sites 3 et 4. Les autres associations sont √©loign√©s d'environ la m√™me distance. Lan√ßons le calcul de la PoMd-m√©trique.

```{r}
pcoa <- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE)
spec_scores <- wascores(pcoa$points, abundance)
ordiplot(vegan::scores(pcoa), type = 't', cex = 1.5)
text(spec_scores, row.names(spec_scores), col = "red", cex = 0.75)
```

On observe en effet que les sites 2 et 3 sont les plus pr√®s. Les sites 3 et 4sont plus √©loign√©s. Les sites 1, 2 et 4 font √† peu pr√®s un triangle √©quilat√©ral, ce qui correspond √† ce √† quoi on devrait s'attendre. Les wa-scores permettent de juxtaposer les esp√®ces sur les sites, pour r√©f√©rence. Le colibri n'est pr√©sent que sur le site 2. Le site 1 est popul√© par des jaseurs et des m√©sanges, et c'est le seul site o√π l'on a observ√© une citelle. On a observ√© des chardonnerets sur les sites 2 et 3. Sur le site 4, on n'a observ√© que des bruants, que l'on a aussi observ√© ailleurs, sauf au site 2.

Le PoMd-non-m√©trique (*non metric dimensional scaling, NMDS*) fonctionne de la m√™me mani√®re que la PoMd-m√©trique, √† la diff√©rence que la distance est bas√©e sur les rangs. √Ä cet √©gard, le site 4 √† une distance de 0.76 du site 3, mais plut√¥t le deuxi√®me plus loin, apr√®s le site 2 et avant le site 1. Utilisons la fonction `metaMDS`.

```{r}
nmds <- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE)
spec_scores <- wascores(nmds$points, abundance)
ordiplot(vegan::scores(nmds), type = 't', cex = 1.5)
text(spec_scores, row.names(spec_scores), col = "red", cex = 0.75)
```

Dans ce cas, entre PoMd-m√©trique et non-m√©trique, les r√©sultats peuvent √™tre interpr√©t√©s de mani√®re similaire.

En ce qui a trait au dauphin,

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='all'}
dolphin_sample <- dolphin %>% sample_n(300)
pcoa_dolphin <- cmdscale(dolphin_sample %>% select(-anatomy) %>% vegdist(method = "euclidean"))
nmds_dolphin <- metaMDS(dolphin_sample %>% select(-anatomy) %>% vegdist(method = "euclidean"), k = 2)

par(mfrow = c(1, 2))
plot(vegan::scores(pcoa_dolphin), pch = 16, col = factor(dolphin_sample$anatomy))
plot(vegan::scores(nmds_dolphin), pch = 16, col = factor(dolphin_sample$anatomy))
```

Pour plus de d√©tails, je vous invite √† vous r√©f√©rer √† [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)) ou de consulter l'excellent site [GUSTA ME](https://mb3is.megx.net/gustame/dissimilarity-based-methods/nmds).

### Conclusion sur l'ordination non contraignante

Lorsque les donn√©es sont euclidiennes, l'analyse en composantes principales (ACP) dervait √™tre utilis√©e. Lorsque la m√©trique est celle du $\chi^2$, on pr√©f√©rera l'analyse de correspondance (AC). Si la m√©trique est autre, le positionnement multidimensionel (PoMd) est pr√©f√©rable. Dans ce dernier cas, si l'on recherche une repr√©sentation simplifi√©e de la distance entre les objets ou variables, on utilisera un PoMd-m√©trique. √Ä l'inverse, si l'on d√©sire une repr√©sentation plus fid√®le au rang des distances, on pr√©f√©rera l'PoMd-non-m√©trique.

## Ordination contraignante

Alors que l'ordination non contraignante vous permet de dresser un protrait de vos variables, l'ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de repr√©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables r√©ponses (par exemple, les esp√®ces observ√©es). 

- L'analyse discriminante n'a fondamentalement qu'une seulement variable r√©ponse, et celle-ci doit d√©crire l'appartenance √† une cat√©gorie.
- L'analyse de redondance sera pr√©f√©r√©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les d√©tails, ainsi que les tenants et aboutissants de ces m√©thodes, sont pr√©sent√©s dans [Numerical Ecology (Legendre et Legendre, 2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).
- L'analyse canonique des corr√©lations sera pr√©f√©r√©e lorsque les variables sont parsem√©es (beaucoup de colonnes avec beaucoup de z√©ros, comme les variables d'abondance).

### Analyse discriminante

Alors que l'analyse en composante principale vise √† pr√©senter la perspective (les axes) selon laquelle les points sont les plus √©clat√©es, l'analyse discriminante, le plus souvent utilis√© dans sa forme lin√©aire (ADL) et quadratique (ADQ), vise √† pr√©senter la perspective selon laquelle les groupes sont les plus √©clat√©s, les groupes formant la variable contraignante. Ces groupes peuvent √™tre connus (e.g. cultivar, r√©gion g√©ographique) ou attribu√©s (exemple: par partitionnement). L'ADL est parfois nomm√©e *analyse canonique de la variance*.

L'AD vise √† repr√©senter des diff√©rences entre des groupes aux moyens de combinaisons lin√©aires (ADL) ou quadratique (ADQ) de variables mesur√©es. Sa repr√©sentation sous forme de biplot permet d'appr√©cier les diff√©rences entre les groupes d'identifier les variables qui sont responsables de la discrimination.

<img src="images/07_ionome-revisited-figure4d.png" alt="" style="width: 600px;"/>

<center>Biplot de distance de l'analyse discriminante des ionomes d'esp√®ces de plantes √† fruits cultiv√©es sauvages et domestiqu√©es,  Source: [Parent et al. (2013)](https://doi.org/10.3389/fpls.2013.00039)</center>

L'ADL a √©t√© d√©velopp√©e par [Fisher (1936)](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract), qui √† titre d'exemple d'application a utilis√© un jeu de donn√©es de dimensions d'iris collect√©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 sp√©cimens d'iris collect√©s en Gasp√©sie (Est du Qu√©bec), ma r√©gion natale (suis-je assez chauvin?). Ce jeu de donn√©es est amplement utilis√© √† titre d'exemple en analyse multivari√©e.

[Williams (1983)](http://www.jstor.org/stable/1937836) a pr√©sent√© les tenants et aboutissants de l'ADL en √©cologie. Tout comme les donn√©es passant pas une ACP doivent suivre une distribution multinormale pour √™tre statistiquement valide, les distributions des groupes dans une ADL doivent √™tre multinormales et les variances des points par groupe doivent √™tre homog√®nes... ce qui est rarement le cas en science. N√©anmoins:

> Heureusement, il y a des √©vidences dans la litt√©rature que certaines d'entre [ces r√®gles] peuvent √™tre transgress√©es mod√©r√©ment sans de grands changement dans les taux de classification. Cette conclusion d√©pends, toutefois, de la s√©v√©rit√© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - [Williams (1983)](http://www.jstor.org/stable/1937836)

L'ADL peut servir autant d'outil d'interpr√©tation que d'outil de classification, c'est √† dire de pr√©dire une cat√©gorie selon les variables (chapitre \@ref(chapitre-ml)). Dans les deux cas, lorsque le nombre de variables approchent le nombre d'observation, les r√©sultats d'une ADL risque d'√™tre difficilement interpr√©tables. Le test appropri√© pour √©valuer l'homod√©n√©it√© de la covariance est le M-test de Box. Ce test est peu document√© dans la litt√©rature, est rarement utilis√© mais a la [r√©putation](https://en.wikiversity.org/wiki/Box%27s_M) d'√™tre particuli√®rement s√©v√®re. 

Il est rare que des donn√©es √©cologiques aient des dispersions (covariances) homog√®nes. Contrairement √† l'ADL, l'ADQ ne demande pas √† ce que les dispersions (covariances) soient homog√®nes. N√©anmoins, l'ADQ ne g√©n√®re ni de scores, ni de loadings: il s'agit d'un outil pour pr√©dire des cat√©gories (classification), non pas d'un outil d'ordination.

#### Application

Utilisons les donn√©es d'iris.

```{r}
data("iris")
```

Testons la multinormalit√© par groupe. Rappelons-nous que pour consid√©rer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent √™tre √©gale ou plus √©lev√©e que 0.05. La fonction `split` s√©pare le tableau en listes et la fonction `map` applique la fonction sp√©cifi√©e √† chaque √©l√©ment de la liste. Cela permet d'effectuer des tests de multinormalit√© sur chacune des esp√®ces d'iris.

```{r}
iris %>% 
  split(.$Species) %>% 
  map(~ mvn(.x %>% select(-Species),
            mvnTest = "mardia")$multivariateNormality)
```

Le test est pass√© pour toutes les esp√®ces. Voyons maintenant l'homog√©n√©it√© de la covariance. Pour ce faire, nous aurons besoin de la fonction `boxM`, disponible avec le module `biotools`. Pour que les covariances soient consid√©r√©es comme √©gales, la p-vaule doit √™tre sup√©rieure √† 0.05.

```{r}
library("heplots")
boxM(iris %>% select(-Species),
     group = iris$Species)
```

On est loin d'un cas o√π les distributions sont homog√®nes. Nous allons n√©anmoins proc√©der √† l'analyse discriminante avec le module ade4. Nous aurons d'abord besoin d'effectuer une ACP avec la fonction `dudi.pca` de ade4 (en sp√©cifiant une mise √† l'√©chelle), que nous projeterons en ADL avec `discrimin`.

```{r}
library("ade4")
iris_pca <- dudi.pca(df = iris %>% select(-Species),
                     scannf = FALSE, # ne pas g√©n√©rer de graphique
                     scale = TRUE)
iris_lda <- discrimin(dudi = iris_pca,
                      fac = iris$Species,
                      scannf = FALSE)
```

La visualisation peut √™tre effectu√©e directement sur l'objet issu de la fonction `discrimin`.

```{r}
plot(iris_lda)
```

Il s'agit toutefois d'une visualisation pour le diagnostic davantage que pour la publication. Si l'objectif est la pubilcation, vous pourriez utiliser la fonction `plotDA` que j'ai con√ßue √† cet effet. J'ai aussi con√ßu une [fonction similaire qui utilise le module graphique de base de R](https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_trad.R).

```{r}
source("https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R")
plotDA(scores = iris_lda$li,
       loadings = iris_lda$fa,
       fac = iris$Species,
       level=0.95,
       facname = "Species",
       propLoadings = 1) 
```

√Ä la diff√©rence de l'ACP, l'ADL maximise la s√©patation des groupes. Nous avions not√© avec l'ACP que les dimensions des p√©tales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu proc√©der directement √† un ADL pour obtenir des conclusions plus directes. Si la longueur des p√©tales permet de distinguer l'esp√®ce *setosa* des deux autres, la largeur des p√©tales permet de distinguer *virginica* et *versicolor*, bien que les nuages de points se superposent. De mani√®re bivari√©e, les r√©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diff√©rence significatives au seuil 0.05. 

<div class="alert alert-block alert-success">**Excercice**. Si l'on effectuait l'ADL sur notre dauphin, avec la colonne `anatomy` comme  variable de regroupement, qu'obtiendrions-nous? Si l'on consi√®re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations?</div>

### Analyse de redondance (RDA)

En anglais, on la nomme *redundancy analysis*, souvent abr√©g√©e RDA. Elle est utilis√©e pour r√©sumer les relations lin√©aires entre des variables r√©ponse et des variables explicatives. La "redondance" se situe dans l'utilisation de deux tableaux de donn√©es contenant de l'information concordante. L'analyse de redondance est une mani√®re √©l√©gante d'effectuer une r√©gresssion lin√©aire multiple, o√π la matrice de valeurs pr√©dites par la r√©gression est assujettie √† une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives √† ceux des variables r√©ponse.

Plus pr√©cis√©ment, une RDA effectue les √©tapes suivantes ([Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)) entre une matrice de variables ind√©pendantes (explicatives) $X$ et une matrice de variables d√©pendantes (r√©ponse) $Y$.

#### 1. R√©gression entre $Y$ et $X$
Pour chacune des variables r√©ponse de $Y$ ($y_1$, $y_2$, , $y_j$), effectuer une r√©gression lin√©aire sur les variables explicatives $X$. 

$$\hat{y}_j = b_j + m_{1, j} \times x_1 + m_{2, j} \times x_2 + ... + m_{i, j} \times x_i$$

$$\hat{y}_j = y_j + y_{res, j}$$

Pour chaque observation ($n$), nous obtenons une s√©rie de valeurs de $\hat{y}_j$ et de $y_{res, j}$. Donc chaque cellule de la matrice $Y$ a ses pendant $\hat{y}$ et $y_{res}$. Nous obtenons ainsi une matrice de pr√©diction $\hat{Y}$ et une matrice des r√©sidus $Y_{res} = Y - \hat{Y}$.

#### 2. Analyse en composantes principales

Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des pr√©dictions $\hat{Y}$. On obtient ainsi ses valeurs et vecteurs propres. Nommons $U$ ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs √† l'√©chelle avant de les retourner √† l'utilisateur. En ordination √©cologique, ces vecteurs mis √† l'√©chelle sont souvent appel√©s les *scores des esp√®ces*, bien qu'il ne s'agisse pas n√©cessairement d'esp√®ces, mais plus g√©n√©ralement des variables de la matrice d√©pendante $Y$.

Il est aussi possible d'effectuer une ACP sur $Y_{res}$.

#### 3. Calculer les scores

Les vecteurs propres $U$ sont utilis√©s pour calculer les *scores des sites*, $Y \times U$, ainsi que les *contraintes de site*  $\hat{Y} \times U$.

#### Application

Nous allons utiliser la fonction `rda` du module vegan. En ce qui a trait aux donn√©es, utilisons les donn√©es varespec (matrice Y) et varechem (matrice X). La fonction `rda` peut fonctionner avec l'interface-formule de R, o√π √† gauche du `~` on retrouve le Y (la matrice de la communaut√© √©cologique, i.e. les abondances d'esp√®ces) contre le X (l), √† gauche, ce qui peut √™tre pratique pour l'analyse d'int√©ractions. Mais pour comparer deux matrices, nous pouvons d√©finir X et Y. Ce qui est m√©langeant, c'est que vegan, contrairement aux conventions, d√©fini X comme √©tant la matrice r√©ponse et Y comme √©tant la matrice explicative.

```{r, fig.height=6, fig.width=10}
vare_rda <- rda(X = varespec, Y = vareclr, scale = FALSE)
par(mfrow = c(1, 2))
ordiplot(vare_rda, scaling = 1, type = "text", main = "Scaling 1: triplot de distance")
ordiplot(vare_rda, scaling = 2, type = "text", main = "Scaling 2: triplot de corr√©lation")
```

La fonction `ordiplot` permet de cr√©er un triplot de base. La repr√©sentation des wascores est r√©put√©e plus robuste (moins susceptible d'√™tre bruit√©e), mais leur interpr√©tation porte √† confusion ([Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)).

**Triplot de distance (scaling 1)**. Les angles entre les variables explicatives repr√©sentent leur corr√©lation (non pas les variables r√©ponse). 

**Triplot de corr√©lation (scaling 2)**. Les angles entre les variables repr√©sentent leurs corr√©lation, que les variables soient r√©ponse ou explicative, ou entre variables r√©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne.

Les triplots montrent que les variables ont toutes un r√¥le important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est compos√© de mani√®re plus marqu√©e par le clr de l'Al et celui du Fe. Le deuxi√®me axe principal est compos√© de mani√®re plus marqu√©e par le clr du S, du P et du K. Le triplot de corr√©lation ne pr√©sente pas de tendance appr√©ciable pour la plupart des esp√®ces, qui ne poss√®dent pas de niche particuli√®re. Toutefois, l'esp√®ce Cladstel, pr√©sente surtout dans les sites 9 et 10, est li√©e √† de basses teneurs en N et √† de faibles valeurs de Baresoil (sol nu). L'esp√®ce Pleuschr est li√©e √† des sols o√π l'on retrouve une grande √©paisseur d'humus, ainsi que des teneurs √©lev√©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble appr√©cier les sols √† bas pH, mais √† faible teneur en Fe et Al. La teneur en N lui semble plus indiff√©rente (son vecteur √©tant presque perpendiculaire).

On pourra personnaliser les graphiques en extrayant les scores.

```{r, fig.height=6, fig.width=6}
scaling <- 2
sites <- vegan::scores(vare_rda, display = "wa", scaling = scaling)
species <- vegan::scores(vare_rda, display = "species", scaling = scaling)
env <- vegan::scores(vare_rda, display = "reg", scaling = scaling)

plot(0, 0, type = "n", xlim = c(-3, 5), ylim = c(-3, 4), asp = 1)
abline(h=0, v = 0, col = "grey80")
text(sites/2, labels = rownames(sites), cex = 0.7, col = "grey50")
text(species/2, labels = rownames(species), col = "green", cex = 0.7)
segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = "blue")
text(env, labels = rownames(env), col = "blue", cex = 1)

```

On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la m√™me mani√®re que pr√©c√©demment. √âtant une collection de r√©gressions, une RDA est en mesure d'effectuer des tests statistiques sur les coefficients de la r√©gression en utilisant des permutations pour tester la signification des coefficients et des axes d'une RDA. On doit n√©anmoins obligatoirement effectuer la RDA avec l'interface formule. L'a variable de gauche'objet √† gauche du `~` peut √™tre une matrice ou un tableau, et celui de droite est d√©fini dans `data`. Le `.` dans l'interface formule signifie "une combinaison lin√©aire de toutes les variables, sans int√©raction".

```{r}
vare_rda <- rda(varespec ~ ., data = vareclr, scale = FALSE)
perm_test_term <- anova(vare_rda, by = "term")
#perm_test_axis <- anova(vare_rda, by = "axis")
```

La signification des axes est difficile √† interpr√©ter. Toutefois, celui des variables pr√©sente un int√©r√™t.

```{r}
perm_test_term
```

La p-value est la probabilit√© que les pentes calcul√©es pour les variables √©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) l'azote, le phosphore, le potassium et l'aluminium.

Dans le cas des matrices d'abondance (ce n'est pas le cas de varespec, constitu√©e de donn√©es de recouvrement), il est pr√©f√©rable avec les RDA de les transformer pr√©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre \@ref(chapitre-explorer)). Une autre option est d'effectuer une RDA sur des matrices d'association en passant par une analyse en coordonn√©es principales ([Legendre et Anderson, 1999](http://www.jstor.org/stable/2657192 )). Enfin, les donn√©es d'abondance √† l'√©tat brutes devraient plut√¥t passer utiliser une analyse canonique des corr√©lations.

### Analyse canonique des correspondances (ACC)

L'analyse canonique des correspondances (*Canonical correspondance analysis*), ACC, a √©t√© √† l'origine con√ßue pour √©tudier les liens entre des variables environnementales et l'abondance (d√©compte) ou l'occurence (pr√©sence-absence) d'esp√®ces ([ter Braak, 1986](https://www.ohio.edu/plantbio/staff/mccarthy/multivariate/terBraak1986.pdf)). L'ACC est √† la RDA ce que la CA est √† l'ACP. Alors que la RDA pr√©serve les distance euclidiennes entre variables d√©pendantes et indpendantes, l'ACC pr√©serve les distances du $\chi^2$. Tout comme l'AC, elle h√©rite du coup une propri√©t√© importate de la distance du $\chi^2$: il y a davantage davantage d'importance aux esp√®ces rares.

L'analyse des correspondances canoniques est souvent utilis√©e dans la litt√©rature, mais dans bien des cas une RDA sur des donn√©es d'abondance transform√©es donnera des r√©sultats davantage int√©rpr√©tables ([Legendre et Gallagher, 2001](https://doi.org/10.1007/s004420100716)).

#### Application

Cet exemple d'application concerne des donn√©es d'abondance. Nous allons cons√©quemment utiliser une CCA avec la fonction `cca`, toujours avec le module vegan.

Les tableaux [`doubs_fish` et `doubs_env`](https://rdrr.io/cran/ade4/man/doubs.html) comprennent respectivement des donn√©es d'abondance d'esp√®ces de poissons et dans diff√©rents environnements de la rivi√®re Doubs (Europe) publi√©es dans [Verneaux. (1973)](https://www.worldcat.org/title/cours-deau-de-franche-comte-massif-du-jura-recherches-ecologiques-sur-le-reseau-hydrographique-du-doubs-essai-de-biotypologie/oclc/496763306) et export√©es du module [`ade4`](https://rdrr.io/rforge/ade4/).

```{r}
data("doubs")
doubs_fish <- doubs$fish
doubs_env <- doubs$env
```

Sur le site no 8, aucun poisson n'a pas √©t√© observ√©. Les observations ne comprenant que des z√©ro doivent √™tre pr√©alablement retir√©es.

```{r}
tot_spec <- doubs_fish %>%
  transmute(tot_spec = apply(., 1, sum))
doubs_fish <- doubs_fish %>%
  filter(tot_spec != 0)
doubs_env <- doubs_env %>%
  filter(tot_spec != 0)
```

De la m√™me mani√®re qu'avec la fonction `rda` de vegan, nous utilisons `cca` pour l'ACC.

```{r}
doubs_cca <- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE)
```

Comparons les r√©sultats

```{r, fig.width=10, fig.height=6}
par(mfrow = c(1, 2))
ordiplot(doubs_cca, scaling = 1, type = "text", main = "CCA - Scaling 1 - Triplot de distance")
ordiplot(doubs_cca, scaling = 2, type = "text", main = "CCA - Scaling 2 - Triplot de corr√©lation")
```

**Triplot de distance (scaling 1)**.

> (1) La projection des variables r√©ponse √† angle droit sur les variables explicatives est une approximation de la r√©ponse sur l'explication. (2) Un objet (site ou r√©ponse) situ√© pr√®s d'une variable explicative est plus susceptible d'avoir le d√©compte `1`. (3) Les distances entre les variables (r√©ponse et explicatives) approximent la distance du $\chi^2$ (traduction adapt√©e de [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)).

**Triplot de corr√©lation (scaling 2)**. 

> (1) La valeur optmiale de l'esp√®ce sur une variable environnementale quantitative peut √™tre obtenue en projetant l'esp√®ce √† angle droit sur la variable. (2) Une esp√®ce se trouvant pr√®s d'une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut `1` pour cette variable. (3) Les distances n'approximent pas la distance du $\chi^2$ (traduction adapt√©e de [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)).


<!--chapter:end:09_ordination.Rmd-->


# Partitionnement {#chapitre-partition}

Placeholder


### √âvaluation d'un partitionnement
#### Score silouhette
#### Indice de Calinski-Harabaz
### Partitionnement non hi√©rarchique
#### Kmeans
##### Application
#### DBSCAN
##### Application
### Partitionnement hi√©rarchique
#### Techniques de partitionnement hi√©rarchique
#### Quel outil de partitionnement hi√©rarchique utiliser?
#### Application
#### Combien de groupes utiliser ?
### Partitionnement hi√©rarchique bas√©e sur la densit√© des points
#### Param√®tres
### Conclusion sur le partitionnement

<!--chapter:end:10_partition.Rmd-->

